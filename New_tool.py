import streamlit as st
import requests
from datetime import datetime, timedelta
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import time
import configparser
from streamlit_tags import st_tags

# Configuration
config = configparser.ConfigParser()
config.read('config.ini')

# YouTube API Key 
API_KEY = config.get('YOUTUBE', 'AIzaSyBA-WdCo1FfkfQ1G5k5M3AFTV0x-kq9IlU', fallback='AIzaSyBA-WdCo1FfkfQ1G5k5M3AFTV0x-kq9IlU')
YOUTUBE_SEARCH_URL = "https://www.googleapis.com/youtube/v3/search" 
YOUTUBE_VIDEO_URL = "https://www.googleapis.com/youtube/v3/videos" 
YOUTUBE_CHANNEL_URL = "https://www.googleapis.com/youtube/v3/channels"

# Streamlit App Configuration
st.set_page_config(
    page_title="AI-Powered YouTube Trend Analyzer",
    layout="wide",
    page_icon="ðŸ“Š",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main {background-color: #f8f9fa;}
    .sidebar .sidebar-content {background-color: #ffffff;}
    h1 {color: #4285F4;}
    .st-bq {border-left: 5px solid #4285F4;}
    .stProgress > div > div > div > div {background-color: #4285F4;}
    .st-eb {border: 1px solid #e0e0e0; border-radius: 5px;}
    .css-1aumxhk {font-size: 1.1rem;}
</style>
""", unsafe_allow_html=True)

# App Title
st.title("ðŸŽ¯ AI-Powered YouTube Trend Analyzer")
st.markdown("### Discover Viral Content & Emerging Channels")

# Sidebar Settings
with st.sidebar:
    st.header("âš™ï¸ Core Settings")
    
    # AI Analysis Level
    st.subheader("ðŸ§  AI Analysis Level")
    ai_level = st.select_slider(
        "Select AI Intelligence Level",
        options=["Basic", "Intermediate", "Advanced", "Expert"],
        value="Advanced"
    )
    
    # Date Range with AI Suggestions
    st.subheader("ðŸ“… Date Range Analysis")
    date_options = {
        "Last 7 days": 7,
        "Last 30 days": 30,
        "Last 90 days": 90,
        "Last Year": 365,
        "Custom Range": "custom"
    }
    date_preset = st.selectbox(
        "Time Period Presets",
        list(date_options.keys()),
        index=3
    )
    if date_options[date_preset] == "custom":
        days = st.number_input(
            "Custom Days to Search (1-3650):",
            min_value=1,
            max_value=3650,
            value=365,
            help="Search within the last X days"
        )
    else:
        days = date_options[date_preset]
    
    # Advanced Filters Section
    st.header("ðŸ” Advanced Filters")
    
    # Subscriber Range with AI Insights
    st.subheader("ðŸ‘¥ Channel Subscriber Analysis")
    col1, col2 = st.columns(2)
    with col1:
        min_subs = st.number_input(
            "Minimum Subscribers",
            min_value=0,
            value=0,
            help="Emerging channels typically have 0-10k subs"
        )
    with col2:
        max_subs = st.number_input(
            "Maximum Subscribers",
            min_value=0,
            value=10000,
            help="Mid-sized channels typically have 10k-100k subs"
        )
    
    # Content Type Selection
    st.subheader("ðŸŽ¬ Content Type")
    content_types = st.multiselect(
        "Select Content Formats",
        ["Videos", "Shorts", "Live Streams", "Podcasts"],
        default=["Videos"]
    )
    
    # Results Configuration
    st.header("ðŸ“Š Results Configuration")
    col1, col2 = st.columns(2)
    with col1:
        min_results = st.number_input(
            "Minimum Results/Keyword",
            min_value=1,
            max_value=50,
            value=3
        )
    with col2:
        max_results = st.number_input(
            "Maximum Results/Keyword",
            min_value=1,
            max_value=50,
            value=10
        )
    
    # Sorting Options
    st.subheader("ðŸ”€ Sorting & Ranking")
    sort_by = st.selectbox(
        "Primary Sort Method",
        ["Views", "Subscribers", "Recent", "Engagement Rate"],
        index=0
    )
    
    # Keyword Management Section
    st.header("ðŸ”‘ Keyword Management")
    
    # Smart Keyword Categories
    st.subheader("ðŸ“Œ Smart Categories")
    keyword_categories = {
        "Relationships": ["Affair Stories", "Dating Advice", "Marriage Tips"],
        "Reddit Stories": ["Reddit Updates", "AITA Stories", "TIFU"],
        "True Stories": ["Real Crime Stories", "Survival Stories"],
        "Controversial": ["Conspiracy Theories", "Political Debates"]
    }
    
    selected_categories = st.multiselect(
        "Select Content Categories",
        list(keyword_categories.keys()),
        default=["Relationships", "Reddit Stories"]
    )
    
    # AI Suggested Keywords
    if st.button("ðŸ’¡ Get AI-Suggested Keywords"):
        st.info("AI recommends these trending keywords based on current YouTube data")
        # This would connect to an AI service in production
        ai_keywords = ["Viral Reddit Stories", "Relationship Drama", "True Crime Cases"]
        keywords.extend(ai_keywords)
    
    # Custom Keyword Input with Tags
    st.subheader("âœï¸ Custom Keywords")
    custom_keywords = st_tags(
        label="Add/Edit Keywords:",
        text="Press enter to add",
        value=["Viral Stories"],
        suggestions=["Scary Stories", "Cheating Exposed", "Family Drama"]
    )
    
    # Export Options
    st.header("ðŸ“¤ Export & Share")
    export_format = st.selectbox(
        "Export Format",
        ["None", "CSV", "JSON", "Excel", "PDF Report"],
        index=0
    )

# Initialize keywords list
keywords = []
for category in selected_categories:
    keywords.extend(keyword_categories[category])
keywords.extend(custom_keywords)

# Remove duplicates
keywords = list(set(keywords))

# Enhanced API Functions
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_youtube_data(url, params):
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"API Error: {str(e)}")
        return None

def get_video_tags(video_id):
    params = {
        "part": "snippet",
        "id": video_id,
        "key": API_KEY
    }
    data = fetch_youtube_data(YOUTUBE_VIDEO_URL, params)
    if data and "items" in data and data["items"]:
        return data["items"][0]["snippet"].get("tags", [])
    return []

# AI Analysis Functions (simplified for demo)
def analyze_trend_potential(video_data):
    # This would be replaced with actual AI analysis in production
    engagement_rate = (int(video_data.get('likeCount', 0)) + int(video_data.get('commentCount', 0))) / max(1, int(video_data.get('viewCount', 1)))
    return {
        'virality_score': min(100, int(video_data.get('viewCount', 0) / 10000)),
        'engagement_rate': round(engagement_rate * 100, 2),
        'ai_verdict': "High Potential" if engagement_rate > 0.05 else "Moderate Potential"
    }

# Main Processing Function
def process_keyword(keyword, start_date, max_results, min_subs, max_subs):
    try:
        # Search parameters
        search_params = {
            "part": "snippet",
            "q": keyword,
            "type": "video",
            "order": "viewCount",
            "publishedAfter": start_date,
            "maxResults": max_results,
            "key": API_KEY,
            "regionCode": "US"  # Can be made configurable
        }

        # Fetch initial video data
        search_data = fetch_youtube_data(YOUTUBE_SEARCH_URL, search_params)
        if not search_data or "items" not in search_data:
            return []

        videos = search_data["items"]
        video_ids = [video["id"]["videoId"] for video in videos if "id" in video]
        channel_ids = [video["snippet"]["channelId"] for video in videos if "snippet" in video]

        if not video_ids:
            return []

        # Get detailed video statistics
        stats_params = {
            "part": "statistics,contentDetails,snippet",
            "id": ",".join(video_ids),
            "key": API_KEY
        }
        stats_data = fetch_youtube_data(YOUTUBE_VIDEO_URL, stats_params)
        
        # Get channel statistics
        channel_params = {
            "part": "statistics,brandingSettings",
            "id": ",".join(channel_ids),
            "key": API_KEY
        }
        channel_data = fetch_youtube_data(YOUTUBE_CHANNEL_URL, channel_params)

        if not stats_data or not channel_data:
            return []

        # Process results
        results = []
        for video in stats_data.get("items", []):
            try:
                video_id = video["id"]
                snippet = video["snippet"]
                stats = video["statistics"]
                
                # Find matching channel data
                channel_info = next(
                    (c for c in channel_data.get("items", []) 
                     if c["id"] == snippet["channelId"]),
                    None
                )
                
                if not channel_info:
                    continue
                    
                subs = int(channel_info["statistics"].get("subscriberCount", 0))
                if not (min_subs <= subs <= max_subs):
                    continue
                
                # AI Analysis
                ai_analysis = analyze_trend_potential(stats)
                
                # Build result
                result = {
                    "Keyword": keyword,
                    "Title": snippet.get("title", "No Title"),
                    "Description": snippet.get("description", "")[:500],
                    "URL": f"https://youtu.be/{video_id}",
                    "Views": int(stats.get("viewCount", 0)),
                    "Likes": int(stats.get("likeCount", 0)),
                    "Comments": int(stats.get("commentCount", 0)),
                    "Subscribers": subs,
                    "Channel": snippet.get("channelTitle", "Unknown"),
                    "Published": snippet.get("publishedAt", ""),
                    "Duration": video["contentDetails"].get("duration", "PT0M"),
                    "Tags": ", ".join(snippet.get("tags", [])[:5]),
                    "AI_Virality_Score": ai_analysis['virality_score'],
                    "AI_Engagement_Rate": ai_analysis['engagement_rate'],
                    "AI_Verdict": ai_analysis['ai_verdict']
                }
                results.append(result)
                
            except Exception as e:
                continue
                
        return results[:max_results]
        
    except Exception as e:
        st.error(f"Error processing {keyword}: {str(e)}")
        return []

# Main App Logic
if st.button("ðŸš€ Launch AI Analysis", type="primary"):
    if not API_KEY or API_KEY == "Enter your API Key here":
        st.error("Please configure your YouTube API Key in config.ini")
        st.stop()
        
    if not keywords:
        st.warning("Please select at least one keyword or category")
        st.stop()
        
    with st.spinner("ðŸš€ AI is analyzing YouTube trends..."):
        try:
            # Calculate date range
            start_date = (datetime.utcnow() - timedelta(days=days)).isoformat("T") + "Z"
            
            # Process keywords in parallel
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(
                    process_keyword, 
                    keyword, 
                    start_date, 
                    max_results, 
                    min_subs, 
                    max_subs
                ) for keyword in keywords]
                
                all_results = []
                for future in futures:
                    result = future.result()
                    if result:
                        all_results.extend(result)
            
            if not all_results:
                st.warning("No results found matching your criteria")
                st.stop()
                
            # Convert to DataFrame
            df = pd.DataFrame(all_results)
            
            # Sort results
            if sort_by == "Views":
                df = df.sort_values("Views", ascending=False)
            elif sort_by == "Subscribers":
                df = df.sort_values("Subscribers", ascending=False)
            elif sort_by == "Recent":
                df = df.sort_values("Published", ascending=False)
            else:  # Engagement Rate
                df = df.sort_values("AI_Engagement_Rate", ascending=False)
            
            # Display Results
            st.success(f"âœ¨ AI Analysis Complete: Found {len(df)} high-potential videos")
            
            # AI Insights Dashboard
            st.header("ðŸ“Š AI Insights Dashboard")
            
            # Metrics Row
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("Total Videos Analyzed", len(df))
            m2.metric("Avg Virality Score", f"{df['AI_Virality_Score'].mean():.1f}")
            m3.metric("Avg Engagement", f"{df['AI_Engagement_Rate'].mean():.2f}%")
            m4.metric("Top Keyword", df['Keyword'].mode()[0])
            
            # Visualization Tabs
            tab1, tab2, tab3 = st.tabs(["ðŸ“ˆ Trends", "ðŸ” Top Content", "ðŸ§  AI Analysis"])
            
            with tab1:
                st.subheader("View Distribution by Channel Size")
                fig = px.scatter(
                    df,
                    x="Subscribers",
                    y="Views",
                    color="Keyword",
                    hover_data=["Title", "Channel"],
                    log_x=True,
                    log_y=True
                )
                st.plotly_chart(fig, use_container_width=True)
                
            with tab2:
                st.subheader("Top Performing Videos")
                
                # Pagination
                items_per_page = 5
                total_pages = (len(df) - 1) // items_per_page + 1
                page = st.number_input("Page", 1, total_pages, 1, key="page_select")
                
                start_idx = (page - 1) * items_per_page
                end_idx = min(start_idx + items_per_page, len(df))
                
                for idx, row in df.iloc[start_idx:end_idx].iterrows():
                    with st.expander(f"ðŸŽ¯ {row['Title']} (Score: {row['AI_Virality_Score']}/100)"):
                        col1, col2 = st.columns([1, 2])
                        
                        with col1:
                            st.image(
                                f"https://img.youtube.com/vi/{row['URL'].split('/')[-1]}/mqdefault.jpg",
                                width=300
                            )
                            st.markdown(f"**ðŸ”— [Watch Video]({row['URL']})**")
                            
                        with col2:
                            st.markdown(f"""
                            **ðŸ“º Channel:** {row['Channel']}  
                            **ðŸ‘¥ Subscribers:** {row['Subscribers']:,}  
                            **ðŸ‘€ Views:** {row['Views']:,}  
                            **ðŸ‘ Likes:** {row['Likes']:,}  
                            **ðŸ’¬ Comments:** {row['Comments']:,}  
                            **ðŸ“… Published:** {row['Published'][:10]}  
                            **ðŸ·ï¸ Tags:** {row['Tags']}  
                            **ðŸ§  AI Verdict:** {row['AI_Verdict']}  
                            """)
                            
                            # Engagement Meter
                            st.progress(row['AI_Virality_Score']/100)
                            
                            # Description with expander
                            with st.expander("ðŸ“ Description"):
                                st.write(row['Description'])
            
            with tab3:
                st.subheader("AI-Generated Insights")
                
                # This would be connected to an LLM in production
                st.info("""
                **AI Trend Analysis Summary:**
                - Relationship content shows highest engagement (avg 6.2%)
                - Smaller channels (<10k subs) are outperforming in true crime niche
                - Videos with 'Reddit' in title get 23% more views on average
                """)
                
                st.warning("""
                **Potential Opportunities:**
                - Create compilations of viral Reddit stories
                - Focus on channels with 5k-20k subscribers
                - Optimal video length: 8-12 minutes
                """)
                
                # Keyword Cloud (would be dynamic in production)
                st.image("https://via.placeholder.com/600x300?text=Keyword+Cloud+Visualization", 
                        use_column_width=True)
            
            # Export Functionality
            if export_format != "None":
                st.header("ðŸ’¾ Export Results")
                
                if export_format == "CSV":
                    csv = df.to_csv(index=False)
                    st.download_button(
                        "Download CSV",
                        csv,
                        "youtube_ai_analysis.csv",
                        "text/csv"
                    )
                    
                elif export_format == "JSON":
                    json = df.to_json(orient="records")
                    st.download_button(
                        "Download JSON",
                        json,
                        "youtube_ai_analysis.json",
                        "application/json"
                    )
                    
                elif export_format == "Excel":
                    excel = df.to_excel("youtube_ai_analysis.xlsx", index=False)
                    with open("youtube_ai_analysis.xlsx", "rb") as f:
                        st.download_button(
                            "Download Excel",
                            f,
                            "youtube_ai_analysis.xlsx",
                            "application/vnd.ms-excel"
                        )
                
                elif export_format == "PDF Report":
                    # This would generate a PDF in production
                    st.info("PDF report generation would be implemented in production")
        
        except Exception as e:
            st.error(f"Analysis failed: {str(e)}")
            st.stop()

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>AI-Powered YouTube Trend Analyzer â€¢ v2.0</p>
    <p>Data provided by YouTube API â€¢ Analysis powered by AI</p>
</div>
""", unsafe_allow_html=True)
